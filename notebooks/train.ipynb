{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e02e390",
   "metadata": {},
   "source": [
    "This notebook contains two main parts: feature engineering and model training\n",
    "\n",
    "**Feature engineering.** Using information from EDA, I conclude the feature engineering as follows:\n",
    "- Use only transactions that its `transac_type` equals to \"TRANSFER\" or \"CASH_OUT\". Only these two types of transaction present fraudulent entries, according to the given data.\n",
    "    - So, at this point, we are kind of building a classifier to predict only \"TRANSFER\" and \"CASH_OUT\" transactions. If fraud in other transaction types were discovered (e.g. by manual flag), the feature engineering part would need to be revised.\n",
    "- remove: `src_acc, dst_acc`. These account numbers don't have quantitative value.\n",
    "- remove: `is_flagged_fraud`. From EDA, this feature has no variance (only 16 rows having values out of ~6M)\n",
    "- new features: `hour_of_day` and `day_of_month`. EDA suggests there are some useful signal from these two features.\n",
    "\n",
    "**Model training.** The task is formulated as a binary classification task. This part is done as follows:\n",
    "1. After the raw data is transformed according to the feature engineering strategy, the data is split into train, dev, and test sets.\n",
    "    - The proportion is approx.: 70%, 10%, and 20% for train, dev, test respectively.\n",
    "    - The train and dev sets are used for manual baseline experiments.\n",
    "    - The train and dev sets will be combined during the hyperparameter tuning (becuase CV will be used here).\n",
    "    - The test set will only be used for reporting the final performance.\n",
    "2. Two baseline models, a logistic regression and a decision tree, are trained.\n",
    "    - The logistic regression is chosen because purely of its simplicity as a baseline.\n",
    "    - The decision tree classifier is selected under this hypothesis: The class imbalance might be handled by the decision tree. Unlike logistic regression, decision trees can partition the feature space to isolate rare patterns well, escpecially if fraudulent transactions follow some distinct rules.\n",
    "4. The decision tree is observed to perform better, thus being proceeded to hyper-parameter tuning with randomized grid-search CV.\n",
    "5. Lastly, the best model, standardization scaler, and the feature list are saved locally to `/models` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5b010-aee1-45b2-8402-6a10dea6e7e3",
   "metadata": {},
   "source": [
    "# 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f0eebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transac_type</th>\n",
       "      <th>amount</th>\n",
       "      <th>src_bal</th>\n",
       "      <th>src_new_bal</th>\n",
       "      <th>dst_bal</th>\n",
       "      <th>dst_new_bal</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>hour_of_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21182.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>229133.94</td>\n",
       "      <td>15325.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5083.00</td>\n",
       "      <td>51513.44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>215310.30</td>\n",
       "      <td>705.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22425.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>311685.89</td>\n",
       "      <td>10835.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6267.00</td>\n",
       "      <td>2719172.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770404</th>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770405</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770406</th>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68488.84</td>\n",
       "      <td>6379898.11</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770407</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770408</th>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6510099.11</td>\n",
       "      <td>7360101.63</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2770409 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        transac_type      amount     src_bal  src_new_bal     dst_bal  \\\n",
       "0           TRANSFER      181.00      181.00          0.0        0.00   \n",
       "1           CASH_OUT      181.00      181.00          0.0    21182.00   \n",
       "2           CASH_OUT   229133.94    15325.00          0.0     5083.00   \n",
       "3           TRANSFER   215310.30      705.00          0.0    22425.00   \n",
       "4           TRANSFER   311685.89    10835.00          0.0     6267.00   \n",
       "...              ...         ...         ...          ...         ...   \n",
       "2770404     CASH_OUT   339682.13   339682.13          0.0        0.00   \n",
       "2770405     TRANSFER  6311409.28  6311409.28          0.0        0.00   \n",
       "2770406     CASH_OUT  6311409.28  6311409.28          0.0    68488.84   \n",
       "2770407     TRANSFER   850002.52   850002.52          0.0        0.00   \n",
       "2770408     CASH_OUT   850002.52   850002.52          0.0  6510099.11   \n",
       "\n",
       "         dst_new_bal  is_fraud  day_of_month  hour_of_day  \n",
       "0               0.00         1             1            1  \n",
       "1               0.00         1             1            1  \n",
       "2           51513.44         0             1            1  \n",
       "3               0.00         0             1            1  \n",
       "4         2719172.89         0             1            1  \n",
       "...              ...       ...           ...          ...  \n",
       "2770404    339682.13         1            32           23  \n",
       "2770405         0.00         1            32           23  \n",
       "2770406   6379898.11         1            32           23  \n",
       "2770407         0.00         1            32           23  \n",
       "2770408   7360101.63         1            32           23  \n",
       "\n",
       "[2770409 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "get_path = lambda :os.path.join('..','data','fraud_mock.csv')\n",
    "\n",
    "df = duckdb.sql(f\"\"\"\n",
    "SELECT \n",
    "    transac_type, \n",
    "    amount,\n",
    "    src_bal,\n",
    "    src_new_bal,\n",
    "    dst_bal,\n",
    "    dst_new_bal,\n",
    "    is_fraud,\n",
    "    CAST((time_ind / 24) + 1 AS INTEGER) AS day_of_month,\n",
    "    MOD(time_ind, 24) AS hour_of_day,\n",
    "FROM '{get_path()}'\n",
    "WHERE transac_type IN ('TRANSFER', 'CASH_OUT')\n",
    "\"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13ce1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transac_type</th>\n",
       "      <th>amount</th>\n",
       "      <th>src_bal</th>\n",
       "      <th>src_new_bal</th>\n",
       "      <th>dst_bal</th>\n",
       "      <th>dst_new_bal</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>hour_of_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>181.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21182.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>229133.94</td>\n",
       "      <td>15325.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5083.00</td>\n",
       "      <td>51513.44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>215310.30</td>\n",
       "      <td>705.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22425.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>311685.89</td>\n",
       "      <td>10835.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6267.00</td>\n",
       "      <td>2719172.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770404</th>\n",
       "      <td>0</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770405</th>\n",
       "      <td>1</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770406</th>\n",
       "      <td>0</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68488.84</td>\n",
       "      <td>6379898.11</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770407</th>\n",
       "      <td>1</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770408</th>\n",
       "      <td>0</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6510099.11</td>\n",
       "      <td>7360101.63</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2770409 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        transac_type      amount     src_bal  src_new_bal     dst_bal  \\\n",
       "0                  1      181.00      181.00          0.0        0.00   \n",
       "1                  0      181.00      181.00          0.0    21182.00   \n",
       "2                  0   229133.94    15325.00          0.0     5083.00   \n",
       "3                  1   215310.30      705.00          0.0    22425.00   \n",
       "4                  1   311685.89    10835.00          0.0     6267.00   \n",
       "...              ...         ...         ...          ...         ...   \n",
       "2770404            0   339682.13   339682.13          0.0        0.00   \n",
       "2770405            1  6311409.28  6311409.28          0.0        0.00   \n",
       "2770406            0  6311409.28  6311409.28          0.0    68488.84   \n",
       "2770407            1   850002.52   850002.52          0.0        0.00   \n",
       "2770408            0   850002.52   850002.52          0.0  6510099.11   \n",
       "\n",
       "         dst_new_bal  is_fraud  day_of_month  hour_of_day  \n",
       "0               0.00         1             1            1  \n",
       "1               0.00         1             1            1  \n",
       "2           51513.44         0             1            1  \n",
       "3               0.00         0             1            1  \n",
       "4         2719172.89         0             1            1  \n",
       "...              ...       ...           ...          ...  \n",
       "2770404    339682.13         1            32           23  \n",
       "2770405         0.00         1            32           23  \n",
       "2770406   6379898.11         1            32           23  \n",
       "2770407         0.00         1            32           23  \n",
       "2770408   7360101.63         1            32           23  \n",
       "\n",
       "[2770409 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since this column only contains two values, we can convert it to binary values.\n",
    "df.loc[df['transac_type'] == 'TRANSFER', 'transac_type'] = 1\n",
    "df.loc[df['transac_type'] == 'CASH_OUT', 'transac_type'] = 0\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74a51d-d2fb-4bf8-ad4b-eaad20e72651",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd89dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import tree\n",
    "\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "features = X.columns.tolist()\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = df['is_fraud'].to_numpy()\n",
    "\n",
    "X_train, X_test_dev, y_train, y_test_dev = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_test_dev,\n",
    "    y_test_dev,\n",
    "    test_size=0.7,\n",
    "    random_state=42,\n",
    "    stratify=y_test_dev\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09be7b4c-5ad2-4f30-b8e5-05d0534f33dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>negatives</th>\n",
       "      <th>positives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>1933537</td>\n",
       "      <td>5749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev</td>\n",
       "      <td>248597</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>580062</td>\n",
       "      <td>1725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  negatives  positives\n",
       "0   train    1933537       5749\n",
       "1     dev     248597        739\n",
       "2    test     580062       1725"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summary = pd.DataFrame({\n",
    "    'dataset': ['train', 'dev', 'test'],\n",
    "    'negatives': [\n",
    "        (y_train == 0).sum(),\n",
    "        (y_dev == 0).sum(),\n",
    "        (y_test == 0).sum()\n",
    "    ],\n",
    "    'positives': [\n",
    "        (y_train == 1).sum(),\n",
    "        (y_dev == 1).sum(),\n",
    "        (y_test == 1).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25eb85c-0f03-4e1f-a43e-e39a005f6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(pred, gold):\n",
    "    \"\"\"\n",
    "    This util function prints confusion matrix and the sklearn's classification report \n",
    "    \"\"\"\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(gold, pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(gold, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026514d-4610-4393-9230-1f4348621037",
   "metadata": {},
   "source": [
    "## 2.1) Baseline: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c463d2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[248562     35]\n",
      " [   375    364]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    248597\n",
      "           1       0.91      0.49      0.64       739\n",
      "\n",
      "    accuracy                           1.00    249336\n",
      "   macro avg       0.96      0.75      0.82    249336\n",
      "weighted avg       1.00      1.00      1.00    249336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_dev_pred = model.predict(X_dev_scaled)\n",
    "report(pred=y_dev_pred, gold=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdd91d-928b-40e9-9ddc-4092949b754d",
   "metadata": {},
   "source": [
    "## 2.2) Baseline: Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80295e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[248521     76]\n",
      " [   115    624]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    248597\n",
      "           1       0.89      0.84      0.87       739\n",
      "\n",
      "    accuracy                           1.00    249336\n",
      "   macro avg       0.95      0.92      0.93    249336\n",
      "weighted avg       1.00      1.00      1.00    249336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_dev_pred = model.predict(X_dev_scaled)\n",
    "report(pred=y_dev_pred, gold=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586435b-194a-4264-ba79-ac04ab2f250c",
   "metadata": {},
   "source": [
    "## 2.3) Decision tree feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9702fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "0.558: src_bal\n",
      "0.181: amount\n",
      "0.118: dst_new_bal\n",
      "0.102: src_new_bal\n",
      "0.014: transac_type\n",
      "0.014: hour_of_day\n",
      "0.010: day_of_month\n",
      "0.003: dst_bal\n"
     ]
    }
   ],
   "source": [
    "feature_importance = dict(zip(model.feature_importances_, features))\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in sorted(feature_importance.items(), key=lambda x: x[0], reverse=True):\n",
    "    print(f\"{importance:.3f}: {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f6c37-256f-445c-a811-3436d46ac337",
   "metadata": {},
   "source": [
    "## 2.4) Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b794eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.822 total time=  21.4s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.841 total time=  19.7s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.837 total time=  22.5s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.831 total time=  20.2s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.851 total time=  20.3s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.842 total time=  19.1s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.860 total time=  18.7s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.858 total time=  21.8s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.845 total time=  20.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.859 total time=  20.6s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.859 total time=  19.3s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.869 total time=  20.2s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.875 total time=  22.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.862 total time=  20.1s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.870 total time=  20.3s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.877 total time=  20.1s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.884 total time=  20.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.886 total time=  21.9s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.880 total time=  19.7s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.884 total time=  20.4s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=2;, score=0.798 total time=  18.9s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=2;, score=0.808 total time=  17.5s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=2;, score=0.820 total time=   9.8s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=2;, score=0.799 total time=  11.3s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=2;, score=0.795 total time=   9.8s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.875 total time=   9.3s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.891 total time=   9.5s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.887 total time=  10.5s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.880 total time=   9.7s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.887 total time=   9.7s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.822 total time=   9.5s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.841 total time=   8.9s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.839 total time=  10.3s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.831 total time=   9.6s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=20, min_samples_leaf=4, min_samples_split=10;, score=0.852 total time=   9.7s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=4, min_samples_split=2;, score=0.790 total time=   9.2s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=4, min_samples_split=2;, score=0.810 total time=  10.4s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=4, min_samples_split=2;, score=0.809 total time=  10.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=4, min_samples_split=2;, score=0.798 total time=  11.4s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=4, min_samples_split=2;, score=0.794 total time=   9.9s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.831 total time=   9.1s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.845 total time=  10.5s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.837 total time=   9.9s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.832 total time=  11.2s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.833 total time=   9.9s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.865 total time=   9.1s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.873 total time=  10.5s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.870 total time=   9.9s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.860 total time=  11.3s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.876 total time=   9.9s\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='log_loss')\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "   'criterion':['entropy', 'log_loss', 'gini'],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(estimator=tree.DecisionTreeClassifier(), \n",
    "                            param_distributions=params, \n",
    "                            n_iter=10,\n",
    "                            cv=5,\n",
    "                            scoring='f1',\n",
    "                            verbose=4)\n",
    "\n",
    "# can combine train and dev set, since the validation is done in CV\n",
    "X_train_cv = np.concatenate([X_train_scaled, X_dev_scaled])\n",
    "y_train_cv = np.concatenate([y_train, y_dev])\n",
    "\n",
    "search.fit(X_train_cv, y_train_cv)\n",
    "print(search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dc4a1-4932-4c3f-9a66-3ee1ef91acba",
   "metadata": {},
   "source": [
    "Performance of the best model over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8395b0cc-2bde-4dc1-b8a4-20ee83330798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search.best_params_={'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'criterion': 'log_loss', 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{search.best_params_=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a02a2d9-a871-4fee-8a62-be12c78ad8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[579885    177]\n",
      " [   237   1488]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    580062\n",
      "           1       0.89      0.86      0.88      1725\n",
      "\n",
      "    accuracy                           1.00    581787\n",
      "   macro avg       0.95      0.93      0.94    581787\n",
      "weighted avg       1.00      1.00      1.00    581787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = tree.DecisionTreeClassifier(**search.best_params_)\n",
    "best_model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "report(pred=y_test_pred, gold=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da05f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..','models','model.pkl'), 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open(os.path.join('..','models','scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open(os.path.join('..','models','features.json'), 'w') as f:\n",
    "    json.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c393b-a30e-4a5e-8458-d2b2d6593d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
